

import { AutoTokenizer } from '../src/transformers.js';
import { getFile } from '../src/utils/hub.js';
import { init, m } from './init.js';

// Initialise the testing environment
init();

// Load test data generated by the python tests
// TODO do this dynamically?
let testsData = await (await getFile('./tests/data/tokenizer_tests.json')).json()

describe('Tokenizers', () => {

    for (let [tokenizerName, tests] of Object.entries(testsData)) {

        it(tokenizerName, async () => {
            let tokenizer = await AutoTokenizer.from_pretrained(m(tokenizerName));

            for (let test of tests) {

                // Test encoding
                let encoded = tokenizer(test.input, {
                    return_tensor: false
                });
                expect(encoded).toEqual(test.encoded);

                // Test decoding
                let decoded_with_special = tokenizer.decode(encoded.input_ids, { skip_special_tokens: false });
                expect(decoded_with_special).toEqual(test.decoded_with_special);

                let decoded_without_special = tokenizer.decode(encoded.input_ids, { skip_special_tokens: true });
                expect(decoded_without_special).toEqual(test.decoded_without_special);
            }
        });
    }
});

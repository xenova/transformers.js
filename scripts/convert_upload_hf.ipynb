{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for remote notebooks, colab etc.\n",
    "!wget https://raw.githubusercontent.com/xenova/transformers.js/main/scripts/requirements.txt\n",
    "!wget https://raw.githubusercontent.com/xenova/transformers.js/main/scripts/convert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt -q\n",
    "!pip install huggingface_hub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, notebook_login, whoami, create_repo, ModelCard, ModelCardData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log in to ðŸ¤— hub**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment when running in vscode\n",
    "# !pip install ipywidgets -q\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = whoami()['name']\n",
    "api = HfApi()\n",
    "print(f\"Logged in to ðŸ¤— as {user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model to convert**  \n",
    "Pick a model from the  [ðŸ¤— Hub](https://huggingface.co/models).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"stabilityai/stablelm-2-1_6b\" # @param {type:\"string\"}\n",
    "user = \"\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the model to ONNX**\n",
    "This might take a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davidrechkemmer/projects/javascript/transformers.js/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "usage: convert.py [-h] --model_id MODEL_ID [--tokenizer_id TOKENIZER_ID]\n",
      "                  [--quantize [QUANTIZE]]\n",
      "                  [--output_parent_dir OUTPUT_PARENT_DIR] [--task TASK]\n",
      "                  [--opset OPSET] [--device DEVICE]\n",
      "                  [--skip_validation [SKIP_VALIDATION]]\n",
      "                  [--per_channel [PER_CHANNEL]]\n",
      "                  [--reduce_range [REDUCE_RANGE]]\n",
      "                  [--output_attentions [OUTPUT_ATTENTIONS]]\n",
      "                  [--split_modalities [SPLIT_MODALITIES]]\n",
      "                  [--trust_remote_code [TRUST_REMOTE_CODE]]\n",
      "                  [--custom_onnx_configs CUSTOM_ONNX_CONFIGS]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model_id MODEL_ID   Model identifier (default: None)\n",
      "  --tokenizer_id TOKENIZER_ID\n",
      "                        Tokenizer identifier (if different to `model_id`)\n",
      "                        (default: None)\n",
      "  --quantize [QUANTIZE]\n",
      "                        Whether to quantize the model. (default: False)\n",
      "  --output_parent_dir OUTPUT_PARENT_DIR\n",
      "                        Path where the converted model will be saved to.\n",
      "                        (default: ./models/)\n",
      "  --task TASK           The task to export the model for. If not specified,\n",
      "                        the task will be auto-inferred based on the model.\n",
      "                        Available tasks depend on the model, but are among:\n",
      "                        ['image-classification', 'text-generation', 'audio-\n",
      "                        xvector', 'stable-diffusion-xl', 'audio-\n",
      "                        classification', 'token-classification', 'automatic-\n",
      "                        speech-recognition', 'zero-shot-object-detection',\n",
      "                        'stable-diffusion', 'question-answering', 'feature-\n",
      "                        extraction', 'image-to-text', 'multiple-choice',\n",
      "                        'text2text-generation', 'object-detection', 'masked-\n",
      "                        im', 'fill-mask', 'mask-generation', 'conversational',\n",
      "                        'text-classification', 'semantic-segmentation',\n",
      "                        'audio-frame-classification', 'image-segmentation',\n",
      "                        'zero-shot-image-classification']. For decoder models,\n",
      "                        use `xxx-with-past` to export the model using past key\n",
      "                        values in the decoder. (default: auto)\n",
      "  --opset OPSET         If specified, ONNX opset version to export the model\n",
      "                        with. Otherwise, the default opset will be used.\n",
      "                        (default: None)\n",
      "  --device DEVICE       The device to use to do the export. (default: cpu)\n",
      "  --skip_validation [SKIP_VALIDATION]\n",
      "                        Whether to skip validation of the converted model\n",
      "                        (default: False)\n",
      "  --per_channel [PER_CHANNEL]\n",
      "                        Whether to quantize weights per channel (default:\n",
      "                        None)\n",
      "  --reduce_range [REDUCE_RANGE]\n",
      "                        Whether to quantize weights with 7-bits. It may\n",
      "                        improve the accuracy for some models running on non-\n",
      "                        VNNI machine, especially for per-channel mode\n",
      "                        (default: None)\n",
      "  --output_attentions [OUTPUT_ATTENTIONS]\n",
      "                        Whether to output attentions from the model. NOTE:\n",
      "                        This is only supported for whisper models right now.\n",
      "                        (default: False)\n",
      "  --split_modalities [SPLIT_MODALITIES]\n",
      "                        Whether to split multimodal models. NOTE: This is only\n",
      "                        supported for CLIP models right now. (default: False)\n",
      "  --trust_remote_code [TRUST_REMOTE_CODE]\n",
      "                        Allows to use custom code for the modeling hosted in\n",
      "                        the model repository. This option should only be set\n",
      "                        for repositoriesyou trust and in which you have read\n",
      "                        the code, as it will execute on your local machine\n",
      "                        arbitrary code present in the model repository.\n",
      "                        (default: False)\n",
      "  --custom_onnx_configs CUSTOM_ONNX_CONFIGS\n",
      "                        Experimental usage: override the default ONNX config\n",
      "                        used for the given model. This argument may be useful\n",
      "                        for advanced users that desire a finer-grained control\n",
      "                        on the export. (default: None)\n"
     ]
    }
   ],
   "source": [
    "# check availabile arguments\n",
    "!python convert.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python convert.py --quantize --model_id $model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.split(\"/\")[-1]\n",
    "model_org = user or model.split(\"/\")[0] # if user is not provided, use the model org\n",
    "repo_id = f\"{model_org}/{model_name}\" # new repo id\n",
    "print(f\"Repo ID: {repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create new repo** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = create_repo(repo_id, exist_ok=True) # creates a new repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the model to the hub\n",
    "api.upload_folder(\n",
    "    folder_path=f\"models/{model}\", # default output path from convert.py\n",
    "    repo_id=repo_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify to your needs\n",
    "card = ModelCard.load(model) # get the old model card\n",
    "card_meta = card.data.to_dict()\n",
    "card_meta['library_name'] = \"transformers.js\"\n",
    "card_meta['tags'] += [\"onnx\"]\n",
    "card_meta = ModelCardData(**card_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add transformers.js modelcard template to existing model card\n",
    "# modify to your needs\n",
    "content = f\"\"\"\n",
    "---\n",
    "{card_meta.to_yaml()}\n",
    "---\n",
    "\n",
    "https://huggingface.co/{model} with ONNX weights to be compatible with Transformers.js.  \n",
    "\n",
    "{card.text}  \n",
    "---\n",
    "  \n",
    "Note: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using [ðŸ¤— Optimum](https://huggingface.co/docs/optimum/index) and structuring your repo like this one (with ONNX weights located in a subfolder named `onnx`).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCard(content).push_to_hub(repo_id) # push the new model card to the hub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
